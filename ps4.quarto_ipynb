{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PSet 4\"\n",
        "author: \"Taha Rashid\"\n",
        "date: \"Feb 7, 2026\"\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "output:\n",
        "  echo: false\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "**Due 02/07 at 5:00PM Central.**\n",
        "\n",
        "\"This submission is my work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: TBR\n",
        "\n",
        "### Github Classroom Assignment Setup and Submission Instructions\n",
        "\n",
        "1.  **Accepting and Setting up the PS4 Assignment Repository**\n",
        "    -   Each student must individually accept the repository for the problem set from Github Classroom (\"ps4\") -- <https://classroom.github.com/a/hWhtcHqH>\n",
        "        -   You will be prompted to select your cnetid from the list in order to link your Github account to your cnetid.\n",
        "        -   If you can't find your cnetid in the link above, click \"continue to next step\" and accept the assignment, then add your name, cnetid, and Github account to this Google Sheet and we will manually link it: <https://rb.gy/9u7fb6>\n",
        "    -   If you authenticated and linked your Github account to your device, you should be able to clone your PS4 assignment repository locally.\n",
        "    -   Contents of PS4 assignment repository:\n",
        "        -   `ps4_template.qmd`: this is the Quarto file with the template for the problem set. You will write your answers to the problem set here.\n",
        "2.  **Submission Process**:\n",
        "    -   Knit your completed solution `ps4.qmd` as a pdf `ps4.pdf`.\n",
        "        -   Your submission does not need runnable code. Instead, you will tell us either what code you ran or what output you got.\n",
        "    -   To submit, push `ps4.qmd` and `ps4.pdf` to your PS4 assignment repository. Confirm on Github.com that your work was successfully pushed.\n",
        "\n",
        "### Grading\n",
        "- You will be graded on what was last pushed to your PS4 assignment repository before the assignment deadline\n",
        "- Problem sets will be graded for completion as: {missing (0%); ✓- (incomplete, 50%); ✓+ (excellent, 100%)}\n",
        "    - The percent values assigned to each problem denote how long we estimate the problem will take as a share of total time spent on the problem set, not the points they are associated with.\n",
        "- In order for your submission to be considered complete, you need to push both your `ps4.qmd` and `ps4.pdf` to your repository. Submissions that do not include both files will automatically receive 50% credit.\n",
        "\n",
        "\n",
        "\\newpage"
      ],
      "id": "e59697ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "alt.renderers.enable('html') \n",
        "\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "alt.renderers.enable(\"png\")\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "id": "42d9dfc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Develop initial scraper and crawler\n"
      ],
      "id": "79a98a86"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set your ID (best practice)\n",
        "my_headers = {\n",
        "    \"User-Agent\": \"MPP30538_Bot (taharashid@uchicago.edu)\"\n",
        "}\n",
        "\n",
        "# URL we want to scrape\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "\n",
        "# Fetch the page from the internet\n",
        "response = requests.get(url, headers=my_headers)\n",
        "\n",
        "# Create a soup (searchabel object)\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n"
      ],
      "id": "a4b81ff5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extracting the right information\n",
        "\n",
        "cards = soup.find_all('li', class_='usa-card')\n",
        "\n",
        "# Grab the first card from our list\n",
        "first_card = cards[0]\n",
        "\n",
        "# 1. Find the <a> tag inside this specific card\n",
        "link_tag = first_card.find('a')\n",
        "\n",
        "# Extract Title (the text)\n",
        "title = link_tag.text.strip()\n",
        "\n",
        "# Extract the Link (the 'href' attribute)\n",
        "link = link_tag.get('href')"
      ],
      "id": "91fa08e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract date\n",
        "date_text = first_card.find('span').text\n",
        "\n",
        "# Test the extraction\n",
        "date_text # got 'February 5, 2026' --> Exctraction successful no errors so far\n",
        "\n",
        "# Extract category\n",
        "category = first_card.find('li', class_='usa-tag').text.strip()\n",
        "\n",
        "category # 'Criminal and Civil Actions' --> Extraction successful"
      ],
      "id": "ad5809c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Consolidate Data\n",
        "\n",
        "all_actions = []\n",
        "\n",
        "for card in cards:\n",
        "    # Get the Title and Link from the <h2>\n",
        "    header = card.find('h2', class_='usa-card__heading')\n",
        "    link_tag = header.find('a')\n",
        "    \n",
        "    title = link_tag.text.strip()\n",
        "    # Glue the base URL to the relative path [cite: 2124, 2136]\n",
        "    link = \"https://oig.hhs.gov\" + link_tag['href']\n",
        "    \n",
        "    # 2. Get the Date from the <span>\n",
        "    date = card.find('span').text.strip()\n",
        "    \n",
        "    # 3. Get the Category from the <li> with class 'usa-tag'\n",
        "    category = card.find('li', class_='usa-tag').text.strip()\n",
        "    \n",
        "    # 4. Save this observation as a dictionary [cite: 1612]\n",
        "    all_actions.append({\n",
        "        \"Title\": title,\n",
        "        \"Date\": date,\n",
        "        \"Category\": category,\n",
        "        \"Link\": link\n",
        "    })"
      ],
      "id": "1e311f72",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.DataFrame(all_actions)\n",
        "\n",
        "display(df) # Everything seems in order"
      ],
      "id": "2d829cc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['Date'].dtype # currently a string object\n",
        "\n",
        "# Convert the 'Date' column to actual datetime objects\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df.dtypes # now date time object\n",
        "\n",
        "display(df)"
      ],
      "id": "5bf06307",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Making the scraper dynamic\n",
        "\n",
        "### 1. Turning the scraper into a function \n",
        "\n",
        "* a. Pseudo-Code\n",
        "\n",
        "We will run a while loop until we reach our desired break point, i.e. the year < 2013. We want all the articles posted after that so first we construct a while loop that takes in the url and finds the information as we did before. We will have an increment +1 to add the page count that we're looping through, since that is how we can move from page 1 url to page 2. On each page, we check the dates of the actions. As soon as we find a date that is older than the month and year the user requested, we stop the loop entirely.\n",
        "\n",
        "We always add a 1-second pause before moving to a new page to prevent server-side block. Once we are done, we save all the information we collected into a CSV file so we don't have to scrape it again\n",
        "\n",
        "\n",
        "* b. Create Dynamic Scraper"
      ],
      "id": "3655ffbf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def scrape_hhs_data(start_year, start_month, run_scraper=False):\n",
        "    if start_year < 2013:\n",
        "        print(\"Year must be >= 2013.\")\n",
        "        return None\n",
        "    if not run_scraper:\n",
        "        return None\n",
        "\n",
        "    start_date = datetime(start_year, start_month, 1)\n",
        "    results = []\n",
        "    page_num = 1\n",
        "    keep_scraping = True\n",
        "    \n",
        "    while keep_scraping:\n",
        "        url = f\"https://oig.hhs.gov/fraud/enforcement/?page={page_num}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        cards = soup.find_all('div', class_='usa-card__container')\n",
        "        if not cards:\n",
        "            break\n",
        "            \n",
        "        for card in cards:\n",
        "            date_container = card.find('div', class_='font-body-sm')\n",
        "            if not date_container:\n",
        "                continue\n",
        "            \n",
        "            # Extract only the date portion\n",
        "            full_text = date_container.get_text(strip=True, separator=\"|\")\n",
        "            date_str = full_text.split(\"|\")[0].strip()\n",
        "            \n",
        "            try:\n",
        "                article_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            if article_date < start_date:\n",
        "                keep_scraping = False\n",
        "                break\n",
        "            \n",
        "            title_tag = card.find('h2').find('a')\n",
        "            title = title_tag.get_text(strip=True)\n",
        "            link = \"https://oig.hhs.gov\" + title_tag['href']\n",
        "            category = full_text.split(\"|\")[-1].strip() if \"|\" in full_text else \"N/A\"\n",
        "            \n",
        "            results.append({'Title': title, 'Date': article_date, 'Category': category, 'Link': link})\n",
        "            \n",
        "        print(f\"Scraped page {page_num}...\")\n",
        "        page_num += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run the final check\n",
        "df_final = scrape_hhs_data(2024, 1, run_scraper=True)\n",
        "\n",
        "if df_final is not None and not df_final.empty:\n",
        "    print(f\"Total actions: {len(df_final)}\")\n",
        "    print(f\"Earliest: {df_final.iloc[-1]['Date'].strftime('%Y-%m-%d')} - {df_final.iloc[-1]['Title']}\")"
      ],
      "id": "85cbe354",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* c. Test Your Code"
      ],
      "id": "409e454b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run the scraper starting from January 2022\n",
        "df_2022 = scrape_hhs_data(2022, 1, run_scraper=True)\n",
        "\n",
        "df_2022.to_csv(\"hhs_enforcement_2022_2026.csv\", index=False)\n"
      ],
      "id": "0693fe55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final 2013 Data Set\n",
        "\n",
        "\n",
        "def scrape_hhs_data(start_year, start_month, run_scraper=False):\n",
        "    # Requirement: Check if year >= 2013\n",
        "    if start_year < 2013:\n",
        "        print(\"Please restrict to year >= 2013, since only enforcement actions after 2013 are listed.\")\n",
        "        return None\n",
        "    \n",
        "    \n",
        "    if not run_scraper:\n",
        "        print(\"Scraper is disabled. Function returning None.\")\n",
        "        return None\n",
        "\n",
        "    start_date = datetime(start_year, start_month, 1)\n",
        "    results = []\n",
        "    page_num = 1\n",
        "    keep_scraping = True\n",
        "    \n",
        "    while keep_scraping:\n",
        "        url = f\"https://oig.hhs.gov/fraud/enforcement/?page={page_num}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        cards = soup.find_all('div', class_='usa-card__container')\n",
        "        if not cards: break\n",
        "            \n",
        "        for card in cards:\n",
        "            date_container = card.find('div', class_='font-body-sm')\n",
        "            if not date_container: continue\n",
        "            \n",
        "            full_text = date_container.get_text(strip=True, separator=\"|\")\n",
        "            date_str = full_text.split(\"|\")[0].strip()\n",
        "            \n",
        "            try:\n",
        "                article_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
        "            except ValueError: continue\n",
        "\n",
        "            if article_date < start_date:\n",
        "                keep_scraping = False\n",
        "                break\n",
        "            \n",
        "            title_tag = card.find('h2').find('a')\n",
        "            results.append({\n",
        "                'Title': title_tag.get_text(strip=True),\n",
        "                'Date': article_date,\n",
        "                'Category': full_text.split(\"|\")[-1].strip(),\n",
        "                'Link': \"https://oig.hhs.gov\" + title_tag['href']\n",
        "            })\n",
        "            \n",
        "        page_num += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save to CSV format\n",
        "    filename = \"enforcement_actions_year_month.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"File saved as {filename}\")\n",
        "    \n",
        "    return df"
      ],
      "id": "1d2ee9c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_2013 = scrape_hhs_data(2013, 1, run_scraper=True)"
      ],
      "id": "9c8c2a2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Plot data based on scraped data\n",
        "\n",
        "### 1. Plot the number of enforcement actions over time"
      ],
      "id": "52e15b12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the full dataset\n",
        "df = pd.read_csv(\"enforcement_actions_year_month.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Filter for Step 3 (Since Jan 2022)\n",
        "df_plot = df[df['Date'] >= '2022-01-01'].copy()\n",
        "\n",
        "# Create a Month-Year column for aggregation\n",
        "df_plot['MonthYear'] = df_plot['Date'].dt.to_period('M').dt.to_timestamp()"
      ],
      "id": "dcd43cd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Plot the number of enforcement actions categorized:\n",
        "\n",
        "* based on \"Criminal and Civil Actions\" vs. \"State Enforcement Agencies\""
      ],
      "id": "f036ff6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggregate count per month\n",
        "overall_counts = df_plot.groupby('MonthYear').size().reset_index(name='n_actions')\n",
        "\n",
        "chart1 = alt.Chart(overall_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('MonthYear:T', title='Month of Action'),\n",
        "    y=alt.Y('n_actions:Q', title='Number of Actions'),\n",
        "    tooltip=['MonthYear', 'n_actions']\n",
        ").properties(\n",
        "    title='Total HHS Enforcement Actions per Month (Since 2022)',\n",
        "    width=600\n",
        ")\n",
        "\n",
        "chart1.display()\n",
        "\n",
        "\n",
        "# Filter for specific categories\n",
        "cat_df = df_plot[df_plot['Category'].isin(['Criminal and Civil Actions', 'State Enforcement Agencies'])]\n",
        "\n",
        "# Group by Month and Category\n",
        "split_counts = cat_df.groupby(['MonthYear', 'Category']).size().reset_index(name='n_actions')\n",
        "\n",
        "chart2 = alt.Chart(split_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('MonthYear:T', title='Month'),\n",
        "    y=alt.Y('n_actions:Q', title='Number of Actions'),\n",
        "    color='Category:N',\n",
        "    tooltip=['MonthYear', 'Category', 'n_actions']\n",
        ").properties(\n",
        "    title='Enforcement Actions by Agency Category',\n",
        "    width=600\n",
        ")\n",
        "\n",
        "chart2.display()"
      ],
      "id": "feddcea5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* based on five topics"
      ],
      "id": "8882536c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def classify_topic(title):\n",
        "    t = title.lower()\n",
        "    if any(word in t for word in ['kickback', 'bribe', 'corruption', 'referral']):\n",
        "        return 'Bribery/Corruption'\n",
        "    if any(word in t for word in ['drug', 'opioid', 'substance', 'pharmacy', 'prescription']):\n",
        "        return 'Drug Enforcement'\n",
        "    if any(word in t for word in ['money laundering', 'securities', 'wire fraud', 'tax', 'bank', 'financial']):\n",
        "        return 'Financial Fraud'\n",
        "    if any(word in t for word in ['medicare', 'medicaid', 'health care', 'medical', 'hospital', 'doctor']):\n",
        "        return 'Health Care Fraud'\n",
        "    return 'Other'\n",
        "\n",
        "# Apply classification only to the Criminal/Civil category\n",
        "df_criminal = df_plot[df_plot['Category'] == 'Criminal and Civil Actions'].copy()\n",
        "df_criminal['Topic'] = df_criminal['Title'].apply(classify_topic)\n",
        "\n",
        "# Aggregate for plotting\n",
        "topic_counts = df_criminal.groupby(['MonthYear', 'Topic']).size().reset_index(name='n_actions')\n",
        "\n",
        "chart3 = alt.Chart(topic_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('MonthYear:T', title='Month'),\n",
        "    y=alt.Y('n_actions:Q', title='Number of Actions'),\n",
        "    color='Topic:N',\n",
        "    tooltip=['MonthYear', 'Topic', 'n_actions']\n",
        ").properties(\n",
        "    title='Criminal and Civil Actions by Topic (Since 2022)',\n",
        "    width=600\n",
        ")\n",
        "\n",
        "chart3.display()"
      ],
      "id": "765044e0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/miniconda3/envs/DAP/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}